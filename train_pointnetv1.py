"""
train_pointnetv1.py - PointNet CFD Flow Predictor Training

PURPOSE:
    Trains a PointNet-based neural network to predict fluid flow fields (velocity and pressure)
    from 3D point cloud representations of CFD simulations. Uses the dataset generated by
    generate_dataset.py to learn flow patterns across various pipe geometries.

USAGE:
    python train_pointnetv1.py

PREREQUISITES:
    1. Generate dataset using generate_dataset.py (data_output/ directory with .npy files)
    2. Conda environment with: torch, wandb, matplotlib, numpy, sklearn
    3. GPU recommended (CUDA) for faster training

CONFIGURATION (Edit DEFAULT_CONFIG, lines 20-34):
    data_dir: Path to .npy dataset files (default: './data_output')
    batch_size: Training batch size (default: 8)
    learning_rate: Initial LR for Adam optimizer (default: 1e-4)
    epochs: Total training epochs (default: 500)
    scaling: Model width multiplier (0.25-2.0, default: 1.0)
    val_split: Validation split ratio (default: 0.2 = 20%)
    vis_frequency: Epoch interval for visualization logging (default: 99)

ARCHITECTURE:
    PointNet segmentation network with:
    - Input: [8 channels] = coords(3) + y_wall(1) + flags(4)
    - Output: [4 channels] = u, v, w, p (velocity + pressure)
    - Features: Local (64) + Global (1024) pooling → Decoder (512→256→128)
    
TRAINING FEATURES:
    - Gradient clipping (max_norm=1.0) for stability
    - Target normalization (velocity by magnitude, pressure by std)
    - ReduceLROnPlateau scheduler (patience=10, factor=0.5)
    - Masked loss on fluid points only (excludes boundaries)
    - W&B logging (auto offline mode if no API key)

OUTPUT:
    - weights/best_model.pth : Best validation loss checkpoint
    - weights/model_<date>_ep<N>.pth : Periodic checkpoints
    - wandb/ : Training logs and visualizations (if online)
    
PERFORMANCE:
    - Expected val loss: ~0.02-0.04 (normalized MSE)
    - Training time: ~6-12 hours on GPU for 500 epochs
    - Visualization: 3D scatter plots of pressure and velocity magnitude

TROUBLESHOOTING:
    - Out of memory: Reduce batch_size or scaling parameter
    - No improvement: Check data quality, increase learning_rate, or increase epochs
    - Gradient explosion: Increase gradient clipping strength (lower max_norm)
"""

"""
train_pointnetv1.py - PointNet CFD Flow Predictor Training
"""

"""
train_pointnetv1.py - PointNet CFD Flow Predictor Training
"""

"""
train_pointnetv1.py - PointNet CFD Flow Predictor Training
"""

import datetime
import os
import glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import wandb
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

# ==========================================
# 1. Configuration
# ==========================================

DEFAULT_CONFIG = {
    "project_name": "ST_CFDAI",
    "data_dir": "./data_output",
    "file_pattern": "*.npy", 
    "batch_size": 32,
    "num_points": 4096,         
    "learning_rate": 1e-3,      
    "epochs": 300,
    "architecture": "PointnetCFD",
    "scaling": 1.0,
    "val_split": 0.15,
    "vis_frequency": 10, 
    "num_workers": 4,           
    "input_channels": 8,
    "output_channels": 4 
}

os.makedirs("weights", exist_ok=True)

# ==========================================
# 2. Memory-Safe Data Loading
# ==========================================

class FluidDataset(Dataset):
    def __init__(self, file_list, num_points=4096):
        self.file_list = file_list
        self.num_points = num_points

    def load_file(self, file_path):
        try:
            # Load the file
            raw = np.load(file_path, allow_pickle=True)
            
            # Case 1: .npz file (Zipped archive)
            if isinstance(raw, np.lib.npyio.NpzFile):
                return raw['data']
                
            # Case 2: 0-D Array wrapping a Dictionary
            if raw.ndim == 0:
                content = raw.item()
                if isinstance(content, dict) and 'data' in content:
                    return content['data']
            
            # Case 3: Raw numpy array
            return raw

        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            return np.zeros((self.num_points, 12), dtype=np.float32)

    def _get_empty_sample(self):
        """Returns a zeroed sample to safely skip bad data"""
        x = torch.zeros((8, self.num_points), dtype=torch.float32)
        y = torch.zeros((4, self.num_points), dtype=torch.float32)
        mask = torch.zeros((self.num_points), dtype=torch.bool)
        return x, y, mask

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        # 1. Load Data
        sample = self.load_file(self.file_list[idx])
        
        # Check 1: Basic Shape
        if sample.ndim != 2 or sample.shape[0] == 0:
            return self._get_empty_sample()

        # Check 2: NaNs or Infs (Corrupted Data)
        if not np.all(np.isfinite(sample)):
            # print(f"Skipping {self.file_list[idx]}: Contains NaN/Inf") # Uncomment to debug
            return self._get_empty_sample()

        # Check 3: Diverged Simulation (Exploded Values)
        # If velocity or pressure is > 1e10, the CFD solver likely diverged.
        if np.max(np.abs(sample[:, 3:7])) > 1e10:
            # print(f"Skipping {self.file_list[idx]}: Exploded values detected") # Uncomment to debug
            return self._get_empty_sample()

        total_points = sample.shape[0]

        # 2. RESAMPLING
        if total_points >= self.num_points:
            choice_idx = np.random.choice(total_points, self.num_points, replace=False)
        else:
            choice_idx = np.random.choice(total_points, self.num_points, replace=True)
        
        sample = sample[choice_idx, :] 

        # 3. Process Features
        coords = sample[:, 0:3].astype(np.float32)
        
        # Normalize Coords
        centroid = np.mean(coords, axis=0)
        coords -= centroid
        dist = np.linalg.norm(coords, axis=1)
        m = np.max(dist)
        if m > 1e-6:
            coords /= m

        features = sample[:, 7:12].astype(np.float32)

        x_in = np.concatenate([coords, features], axis=1)
        x_in = x_in.transpose(1, 0) 

        # 4. Process Targets (u, v, w, p)
        # Use float64 for stats calculation to prevent overflow warnings
        y_out_raw = sample[:, 3:7].astype(np.float64)
        
        # Normalize Velocity
        vel_mag = np.linalg.norm(y_out_raw[:, 0:3], axis=1, keepdims=True)
        max_vel = np.max(vel_mag)
        
        # Normalize Pressure
        p_mean = np.mean(y_out_raw[:, 3])
        p_std = np.std(y_out_raw[:, 3])

        # Apply Normalization (Back to float32)
        y_out = sample[:, 3:7].astype(np.float32)

        if max_vel > 1e-6:
            y_out[:, 0:3] /= max_vel
        
        if p_std > 1e-6:
            y_out[:, 3] = (y_out[:, 3] - p_mean) / p_std
            
        y_out = y_out.transpose(1, 0)

        # 5. Mask
        fluid_mask = sample[:, 8].astype(bool)

        return torch.from_numpy(x_in), torch.from_numpy(y_out), torch.from_numpy(fluid_mask)

# ==========================================
# 3. Model
# ==========================================

class PointNetFluid(nn.Module):
    def __init__(self, input_channels=8, output_channels=4, scaling=1.0):
        super(PointNetFluid, self).__init__()
        s = scaling
        self.conv1 = nn.Conv1d(input_channels, int(64*s), 1)
        self.bn1 = nn.BatchNorm1d(int(64*s))
        self.conv2 = nn.Conv1d(int(64*s), int(64*s), 1)
        self.bn2 = nn.BatchNorm1d(int(64*s))
        self.conv3 = nn.Conv1d(int(64*s), int(64*s), 1)
        self.bn3 = nn.BatchNorm1d(int(64*s))
        self.conv4 = nn.Conv1d(int(64*s), int(128*s), 1)
        self.bn4 = nn.BatchNorm1d(int(128*s))
        self.conv5 = nn.Conv1d(int(128*s), int(1024*s), 1)
        self.bn5 = nn.BatchNorm1d(int(1024*s))

        # Segmentation Head
        self.conv6 = nn.Conv1d(int(1024*s) + int(64*s), int(512*s), 1)
        self.bn6 = nn.BatchNorm1d(int(512*s))
        self.conv7 = nn.Conv1d(int(512*s), int(256*s), 1)
        self.bn7 = nn.BatchNorm1d(int(256*s))
        self.conv8 = nn.Conv1d(int(256*s), int(128*s), 1)
        self.bn8 = nn.BatchNorm1d(int(128*s))
        self.conv9 = nn.Conv1d(int(128*s), output_channels, 1)

    def forward(self, x):
        n_pts = x.size(2)
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        local_feat = x 

        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))

        global_feat = F.max_pool1d(x, n_pts)
        global_feat = global_feat.expand(-1, -1, n_pts)

        x = torch.cat([local_feat, global_feat], dim=1)

        x = F.relu(self.bn6(self.conv6(x)))
        x = F.relu(self.bn7(self.conv7(x)))
        x = F.relu(self.bn8(self.conv8(x)))
        x = self.conv9(x)
        return x

# ==========================================
# 4. Utilities
# ==========================================

def create_comparison_plot(coords, target, pred, title_prefix):
    if coords.shape[0] > 2000:
        idx = np.random.choice(coords.shape[0], 2000, replace=False)
        coords = coords[idx]
        target = target[idx]
        pred = pred[idx]

    fig = plt.figure(figsize=(10, 5))
    ax1 = fig.add_subplot(1, 2, 1, projection='3d')
    sc1 = ax1.scatter(coords[:,0], coords[:,1], coords[:,2], c=target, cmap='jet', s=5, alpha=0.7)
    ax1.set_title(f"Truth: {title_prefix}")
    ax1.axis('off')
    
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')
    sc2 = ax2.scatter(coords[:,0], coords[:,1], coords[:,2], c=pred, cmap='jet', s=5, alpha=0.7)
    ax2.set_title(f"Pred: {title_prefix}")
    ax2.axis('off')
    plt.tight_layout()
    return fig

def log_visualizations(model, val_loader, device, epoch):
    model.eval()
    try:
        inputs, targets, masks = next(iter(val_loader))
    except StopIteration:
        return

    inputs, targets = inputs.to(device), targets.to(device)
    
    with torch.no_grad():
        preds = model(inputs)

    idx = 0
    coords = inputs[idx].cpu().numpy().transpose(1, 0)[:, 0:3]
    y_true = targets[idx].cpu().numpy().transpose(1, 0)
    y_pred = preds[idx].cpu().numpy().transpose(1, 0)
    
    fig_p = create_comparison_plot(coords, y_true[:, 3], y_pred[:, 3], "Pressure")
    v_true = np.linalg.norm(y_true[:,0:3], axis=1)
    v_pred = np.linalg.norm(y_pred[:,0:3], axis=1)
    fig_v = create_comparison_plot(coords, v_true, v_pred, "Velocity Mag")

    wandb.log({"Vis/Pressure": wandb.Image(fig_p), "Vis/Velocity": wandb.Image(fig_v), "epoch": epoch})
    plt.close(fig_p)
    plt.close(fig_v)

# ==========================================
# 5. Main Loop
# ==========================================

def get_file_list(config):
    pattern = os.path.join(config.data_dir, config.file_pattern)
    files = sorted(glob.glob(pattern))
    if not files:
        raise ValueError(f"No files found in {pattern}")
    return files

def main():
    wandb.init(project=DEFAULT_CONFIG["project_name"], config=DEFAULT_CONFIG, mode="online")
    config = wandb.config 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    all_files = get_file_list(config)
    train_files, val_files = train_test_split(all_files, test_size=config.val_split, random_state=42)
    
    train_ds = FluidDataset(train_files, num_points=config.num_points)
    val_ds = FluidDataset(val_files, num_points=config.num_points)

    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, 
                              num_workers=config.num_workers, pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False, 
                            num_workers=config.num_workers, pin_memory=True, persistent_workers=True)

    model = PointNetFluid(input_channels=config.input_channels, 
                          output_channels=config.output_channels, 
                          scaling=config.scaling).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)
    criterion = nn.MSELoss(reduction='none')

    print(f"Training on {len(train_files)} files. Point count fixed to {config.num_points}.")

    best_val_loss = float('inf')

    for epoch in range(config.epochs):
        model.train()
        train_loss_accum = 0.0
        count = 0
        
        for i, (inputs, targets, masks) in enumerate(train_loader):
            inputs = inputs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)
            masks = masks.to(device, non_blocking=True)
            
            # --- DEBUG: CHECK DATA INTEGRITY (Run once per epoch) ---
            if i == 0 and epoch == 0:
                print("\n--- SANITY CHECK (Batch 0) ---")
                print(f"Input Range: {inputs.min().item():.3f} to {inputs.max().item():.3f}")
                print(f"Target Range: {targets.min().item():.3f} to {targets.max().item():.3f}")
                print(f"Fluid Points: {masks.sum().item()} / {masks.numel()} ({masks.sum().item()/masks.numel()*100:.1f}%)")
                print("------------------------------\n")
            # ---------------------------------------------------------

            optimizer.zero_grad()
            outputs = model(inputs)

            loss_raw = criterion(outputs, targets) 
            
            # Robust Loss Calculation
            valid_points = masks.sum()
            if valid_points > 0:
                loss = loss_raw.mean(dim=1)[masks].mean()
            else:
                loss = loss_raw.mean()

            if not torch.isnan(loss):
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss_accum += loss.item()
                count += 1
                wandb.log({"batch_loss": loss.item()})
            else:
                print(f"NaN loss detected at batch {i}")

        avg_train = train_loss_accum / max(count, 1)

        model.eval()
        val_loss_accum = 0.0
        val_count = 0
        with torch.no_grad():
            for inputs, targets, masks in val_loader:
                inputs = inputs.to(device, non_blocking=True)
                targets = targets.to(device, non_blocking=True)
                masks = masks.to(device, non_blocking=True)
                
                outputs = model(inputs)
                loss_raw = criterion(outputs, targets)
                
                if masks.sum() > 0:
                    val_loss = loss_raw.mean(dim=1)[masks].mean()
                else:
                    val_loss = loss_raw.mean()
                
                if not torch.isnan(val_loss):
                    val_loss_accum += val_loss.item()
                    val_count += 1

        avg_val = val_loss_accum / max(val_count, 1)
        scheduler.step(avg_val)

        print(f"Epoch {epoch+1} | Train: {avg_train:.5f} | Val: {avg_val:.5f}")
        wandb.log({"train_loss": avg_train, "val_loss": avg_val, "epoch": epoch, "lr": optimizer.param_groups[0]['lr']})

        if avg_val < best_val_loss:
            best_val_loss = avg_val
            torch.save(model.state_dict(), "weights/best_model.pth")
            
        if epoch % config.vis_frequency == 0:
            log_visualizations(model, val_loader, device, epoch)

    wandb.finish()

if __name__ == "__main__":
    main()